model:
  name: "LSTM"
  hidden_size: 128
  num_layers: 2
  dropout: 0.2
  bidirectional: false

training:
  learning_rate: 0.001
  weight_decay: 0.0001
  batch_size: 64
  max_epochs: 50
  patience: 15
  scheduler: "reduce_on_plateau"
  scheduler_patience: 5
  scheduler_factor: 0.5
  gradient_clip_val: 1.0
